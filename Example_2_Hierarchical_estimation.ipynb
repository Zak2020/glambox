{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import glam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Three small groups with difference in gaze bias\n",
    "\n",
    "In some research settings, the total amount of data can be quite small, conflicting with the amounts of data usually necessary to obtain reliable and precise parameter estimates from diffusion models. To alleviate this, hierarchical approaches can be made, where data from individuals inform estimates of parameters of the group, to the extent that they are informative. Such hierarchical parameter estimation can greatly improve parameter estimation in face of limited amounts of data (cf. the HDDM toolbox for hierarchical modeling using the Drift Diffusion Model).\n",
    "\n",
    "Here, we will simulate a clinical setting, in which different patient groups are to be compared on their gaze biases during a simple value-based choice task, that includes eye tracking. One could imagine that the data that can be generated within this setting is limited on at least two accounts:\n",
    "\n",
    "1. The number of patients with certain conditions that are available for the experiment might be low\n",
    "2. The number of trials that can be performed by each individual might also be low, for clinical reasons (e.g., patients feel exhausted earlier, ...)\n",
    "\n",
    "Therefore, the dataset we generate will contain only low numbers of individuals within each group, and only 50 trials (roughly corresponding to a 15-minute experimental) per participant.\n",
    "\n",
    "We will then estimate model parameters in a hierarchical fashion, and compare the group level gaze bias parameters between groups.\n",
    "\n",
    "## Simulate data\n",
    "\n",
    "We simulate the data such that only the gaze bias parameter $\\gamma$ differs between the groups, with means of 1.0 (no gaze bias), 0.3 (moderate gaze bias) and -0.5 (strong gaze bias), respectively. All other parameters are drawn from the same distributions across groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1523)\n",
    "\n",
    "N = dict(group1=7,\n",
    "         group2=12,\n",
    "         group3=5)\n",
    "\n",
    "v = dict(group1=np.clip(np.random.normal(loc=0.0001, scale=0.000025, size=N['group1']), a_min=0, a_max=None),\n",
    "         group2=np.clip(np.random.normal(loc=0.0001, scale=0.000025, size=N['group2']), a_min=0, a_max=None),\n",
    "         group3=np.clip(np.random.normal(loc=0.0001, scale=0.000025, size=N['group3']), a_min=0, a_max=None))\n",
    "\n",
    "s = dict(group1=np.clip(np.random.normal(loc=0.006, scale=0.001, size=N['group1']), a_min=0, a_max=None),\n",
    "         group2=np.clip(np.random.normal(loc=0.006, scale=0.001, size=N['group2']), a_min=0, a_max=None),\n",
    "         group3=np.clip(np.random.normal(loc=0.006, scale=0.001, size=N['group3']), a_min=0, a_max=None))\n",
    "\n",
    "gamma = dict(group1=np.clip(np.random.normal(loc= 1.0, scale=0.2, size=N['group1']), a_min=None, a_max=1),\n",
    "             group2=np.clip(np.random.normal(loc= 0.3, scale=0.4, size=N['group2']), a_min=None, a_max=1),\n",
    "             group3=np.clip(np.random.normal(loc=-0.5, scale=0.2, size=N['group3']), a_min=None, a_max=1))\n",
    "\n",
    "tau = dict(group1=np.clip(np.random.normal(loc=1, scale=0.1, size=N['group1']), a_min=0, a_max=None),\n",
    "           group2=np.clip(np.random.normal(loc=1, scale=0.1, size=N['group2']), a_min=0, a_max=None),\n",
    "           group3=np.clip(np.random.normal(loc=1, scale=0.1, size=N['group3']), a_min=0, a_max=None))\n",
    "\n",
    "n_trials = 50\n",
    "n_items  = 3\n",
    "\n",
    "model = glam.GLAM()\n",
    "\n",
    "groups = ['group1', 'group2', 'group3']\n",
    "for group in groups:\n",
    "    model.simulate_group(kind='individual', n_individuals=N[group], n_trials=n_trials, n_items=n_items,\n",
    "                         parameters=dict(v=v[group], gamma=gamma[group], s=s[group], t0=np.zeros(N[group]), tau=tau[group]),\n",
    "                         label=group)\n",
    "\n",
    "data = model.data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the generating parameters look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 10\n",
    "fig, axs = plt.subplots(1, 4, figsize=(14, 3), sharey=True)\n",
    "for p, (parameter, name, bins) in enumerate(zip([v, s, gamma, tau],\n",
    "                                                ['v', 's', 'gamma', 'tau'],\n",
    "                                                [np.linspace(0, 0.0002, n_bins + 1), np.linspace(0, 0.02, n_bins + 1),\n",
    "                                                 np.linspace(-1, 1, n_bins + 1), np.linspace(0, 2, n_bins + 1)])):\n",
    "    for g, group in enumerate(groups):\n",
    "        axs[p].hist(parameter[group],\n",
    "                    bins=bins,\n",
    "                    color='C{}'.format(g),\n",
    "                    label=group,\n",
    "                    alpha=0.5)\n",
    "        axs[p].set_xlabel(name)\n",
    "        axs[p].set_ylabel('Frequency')\n",
    "        axs[p].spines['top'].set_visible(False)\n",
    "        axs[p].spines['right'].set_visible(False)\n",
    "        \n",
    "axs[0].legend(frameon=False)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the generated data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glam.plot_fit(data=data,\n",
    "              predictions=[data.loc[data['condition'] == group] for group in groups],\n",
    "              prediction_labels=groups);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the data were generated only with different gaze biases, the groups behaviourally also differ in response times (Panel A; with group one being slower as groups one and three) and choice accuracy (Panel B; group three clearly less accurate than the other, and group one more accurate than the others). As was to be expected, we can also observe behavioural differences in gaze influence measures (Panels C and D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical model\n",
    "\n",
    "We now set up the hierarchical model, where we assume that every group has a distinct average gaze bias, which we then estimate.\n",
    "\n",
    "![glam_hierarchical_illustration](figures/hierarchical_glam_illustration.png)\n",
    "\n",
    "!! Include a section to talk about priors. !!\n",
    "\n",
    "Since we use `PyMC3` to build our models and infer parameter estimates, we can use all inference techniques supplied by `PyMC3`. \n",
    "\n",
    "### MCMC\n",
    "\n",
    "As before, we can use a traditional Markov-Chain-Monte-Carlo approach, with the Metropolis-Hastings algorithm. While this algorithm produces highly correlated samples, it can still obtain a reasonable number of effective samples, because it runs relatively fast. However, convergence should definitely diagnosed. We recommend to draw as many samples as possible within your time frame, and especially allowing the sampler to tune itself to make sure it is exploring the relevant region of the parameter space. If you observe slow drifts in the traces, consider increasing the number of tuning samples further. Even though the sampler can be parallelized, each chain running in parallel needs to draw the same number of tuning samples, so this process unfortunately cannot be cut short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hierarchical_mcmc = glam.GLAM(data=model.data.copy())\n",
    "model_hierarchical_mcmc.make_model(kind='hierarchical',\n",
    "                                   depends_on=dict(gamma='condition'),\n",
    "                                   t0_val=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_hierarchical_mcmc.fit(method='MCMC', step=pm.DEMetropolis, draws=5000, tune=20000, cores=16, chains=250, seed=1556)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! Brief paragraph about convergence and convergence diagnostics? !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pm.traceplot(model_hierarchical_mcmc.trace[0],\n",
    "             varnames=['v_mu', 'v_sd', 'v',\n",
    "                       's_mu', 's_sd', 's',\n",
    "                       'tau_mu', 'tau_sd', 'tau',\n",
    "                       'gamma_group1_mu', 'gamma_group1_sd', 'gamma_group1',\n",
    "                       'gamma_group2_mu', 'gamma_group2_sd', 'gamma_group2',\n",
    "                       'gamma_group3_mu', 'gamma_group3_sd', 'gamma_group3'],\n",
    "             combined=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glam.plots.plot_node(model_hierarchical_mcmc, parameter='gamma',\n",
    "                     comparisons=[('group1', 'group2'), ('group1', 'group3'), ('group2', 'group3')]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean individual estimated parameters\n",
    "model_hierarchical_mcmc.estimates.groupby('condition')['gamma'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group level estimates\n",
    "model_hierarchical_mcmc.estimates.groupby('condition')['gamma_mu'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean individual generating parameters\n",
    "{group: gamma[group].mean() for group in groups}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! Evaluate MCMC output !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Inference\n",
    "\n",
    "Alternatively, we can use the methods for variational inference that ship with PyMC3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hierarchical_vi = glam.GLAM(data=model.data.copy())\n",
    "model_hierarchical_vi.make_model(kind='hierarchical',\n",
    "                                 depends_on=dict(gamma='condition'),\n",
    "                                 t0_val=0)\n",
    "model_hierarchical_vi.fit(method='VI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the variational inference methods are faster than the MCMC methods, but they can also yield less accurate results. For one, they assume that each parameter's posterior distribution can be well approximated by a normal distribution. Obviously this assumption can lead to very wrong inferences, when violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter estimates\n",
    "\n",
    "After the model has been fit, the same analysis as before can be performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glam.plots.plot_node(model_hierarchical_vi, parameter='gamma',\n",
    "                     comparisons=[('group1', 'group2'), ('group1', 'group3'), ('group2', 'group3')]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean individual estimated parameters\n",
    "model_hierarchical_vi.estimates.groupby('condition')['gamma'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group level estimates\n",
    "model_hierarchical_vi.estimates.groupby('condition')['gamma_mu'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean individual generating parameters\n",
    "{group: gamma[group].mean() for group in groups}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated mean gaze bias parameters of the groups closely resemble the (usually unknown, but in this simulated case known) data-generating parameters.\n",
    "\n",
    "We could now conclude, that the groups all differ in their gaze bias: Group 1 has a lower gaze bias (higher $\\gamma$) than Group 2 (Mean difference = 0.64, 95% HPD = [0.44, 0.85]) and Group 3 (Mean difference = 1.57, 95% HPD = [1.47, 1.68]), whereas Group 2 has a lower gaze bias than Group 3 (Mean difference = 0.93, 95% HPD = [0.72, 1.16])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*INTERNAL: THE POSTERIORS FROM MCMC AND ADVI LOOK PRETTY SIMILAR. I SUSPECT THIS IS DUE TO SOMETHING NOT CONVERGING IN MCMC..*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
